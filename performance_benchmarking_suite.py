#!/usr/bin/env python3
"""
Performance Benchmarking Suite for WINCASA Database Optimizations
================================================================

Comprehensive benchmarking of database performance optimizations including:
1. Connection pooling vs direct connections
2. Query result caching effectiveness  
3. SQL query optimization improvements
4. End-to-end retrieval mode performance
5. Complex JOIN operation optimizations

Generates detailed performance reports and recommendations.
"""

import statistics
import time
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict

from sql_query_optimizer import SQLQueryOptimizer
from query_result_cache import get_query_cache
from gemini_llm import get_gemini_llm


@dataclass
class BenchmarkResult:
    """Single benchmark test result."""
    test_name: str
    execution_time: float
    success: bool
    result_size: int
    error_message: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return asdict(self)


@dataclass
class PerformanceComparison:
    """Comparison between baseline and optimized performance."""
    baseline_time: float
    optimized_time: float
    improvement_factor: float = 0.0
    improvement_percentage: float = 0.0
    
    def __post_init__(self):
        """Calculate improvement metrics."""
        if self.optimized_time > 0:
            self.improvement_factor = self.baseline_time / self.optimized_time
            self.improvement_percentage = ((self.baseline_time - self.optimized_time) / self.baseline_time) * 100
        else:
            self.improvement_factor = 1.0
            self.improvement_percentage = 0.0
    
    def get_improvement_level(self) -> str:
        """Get improvement level description."""
        if self.improvement_factor >= 3.0:
            return "ðŸš€ EXCELLENT"
        elif self.improvement_factor >= 2.0:
            return "âš¡ SIGNIFICANT"
        elif self.improvement_factor >= 1.5:
            return "âœ¨ GOOD"
        elif self.improvement_factor >= 1.1:
            return "ðŸ“ˆ MINOR"
        else:
            return "âšª MINIMAL"


class DatabasePerformanceBenchmark:
    """
    Comprehensive database performance benchmarking suite.
    
    Tests various optimization strategies and provides detailed
    performance analysis and recommendations.
    """
    
    def __init__(self):
        """Initialize benchmark suite."""
        self.sql_optimizer = SQLQueryOptimizer()
        self.query_cache = get_query_cache(max_size=100, default_ttl_seconds=300)
        self.benchmark_results: List[BenchmarkResult] = []
        
        # Standard test queries for WINCASA database
        self.test_queries = {
            'simple_count': "SELECT COUNT(*) FROM BEWOHNER",
            'simple_filter': "SELECT BNAME FROM BEWOHNER WHERE BWO < 100",
            'address_search': "SELECT BNAME, BSTR FROM BEWOHNER WHERE BSTR LIKE '%Marien%'",
            'simple_join': """
                SELECT b.BNAME, o.OSTR 
                FROM BEWOHNER b 
                JOIN OBJEKTE o ON b.ONR = o.ONR 
                WHERE b.BWO < 50
            """,
            'complex_join': """
                SELECT b.BNAME, o.OSTR, w.WHG_NR, k.SALDO
                FROM BEWOHNER b
                JOIN OBJEKTE o ON b.ONR = o.ONR
                JOIN WOHNUNG w ON o.ONR = w.ONR
                JOIN KONTEN k ON o.ONR = k.ONR
                WHERE b.BSTR LIKE '%Marien%'
            """,
            'large_result_set': "SELECT * FROM BEWOHNER",
            'aggregation': """
                SELECT COUNT(*) as bewohner_count, 
                       AVG(CASE WHEN w.QMWFL > 0 THEN w.QMWFL END) as avg_size
                FROM BEWOHNER b
                JOIN OBJEKTE o ON b.ONR = o.ONR
                JOIN WOHNUNG w ON o.ONR = w.ONR
            """,
            'subquery': """
                SELECT BNAME 
                FROM BEWOHNER 
                WHERE ONR IN (
                    SELECT ONR FROM OBJEKTE WHERE OSTR LIKE '%Hauptstr%'
                )
            """
        }
        
        print("ðŸŽ¯ Database Performance Benchmark Suite Initialized")
        print(f"ðŸ“Š Test queries: {len(self.test_queries)}")
        print(f"ðŸ”§ Optimization tools: SQL Optimizer, Query Cache")
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """
        Run comprehensive performance benchmark.
        
        Returns:
            Detailed benchmark results and analysis
        """
        print("\n" + "=" * 80)
        print("ðŸš€ COMPREHENSIVE DATABASE PERFORMANCE BENCHMARK")
        print("=" * 80)
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'test_categories': {},
            'summary': {},
            'recommendations': []
        }
        
        # Test Category 1: SQL Query Optimization
        print("\nðŸ“Š CATEGORY 1: SQL Query Optimization")
        print("-" * 60)
        results['test_categories']['sql_optimization'] = self._benchmark_sql_optimization()
        
        # Test Category 2: Query Result Caching
        print("\nðŸ’¾ CATEGORY 2: Query Result Caching")
        print("-" * 60)
        results['test_categories']['query_caching'] = self._benchmark_query_caching()
        
        # Test Category 3: Complex JOIN Performance
        print("\nðŸ”— CATEGORY 3: Complex JOIN Performance")
        print("-" * 60)
        results['test_categories']['join_performance'] = self._benchmark_join_performance()
        
        # Test Category 4: Large Result Set Handling
        print("\nðŸ“ˆ CATEGORY 4: Large Result Set Handling")
        print("-" * 60)
        results['test_categories']['large_results'] = self._benchmark_large_results()
        
        # Generate summary and recommendations
        results['summary'] = self._generate_summary(results['test_categories'])
        results['recommendations'] = self._generate_recommendations(results['test_categories'])
        
        return results
    
    def _benchmark_sql_optimization(self) -> Dict[str, Any]:
        """Benchmark SQL query optimization effectiveness."""
        optimization_results = {}
        
        for query_name, query in self.test_queries.items():
            print(f"\nðŸ§ª Testing {query_name}")
            
            # Test original query performance
            baseline_time = self._time_mock_query_execution(query)
            
            # Optimize query
            optimization_result = self.sql_optimizer.optimize_query(query)
            optimized_query = optimization_result.optimized_query
            
            # Test optimized query performance
            optimized_time = self._time_mock_query_execution(optimized_query)
            
            # Calculate improvement
            comparison = PerformanceComparison(baseline_time, optimized_time)
            
            print(f"   Original: {baseline_time:.3f}s")
            print(f"   Optimized: {optimized_time:.3f}s")
            print(f"   Improvement: {comparison.get_improvement_level()} ({comparison.improvement_factor:.1f}x)")
            
            if optimization_result.optimizations_applied:
                print(f"   Optimizations: {[opt.value for opt in optimization_result.optimizations_applied]}")
            
            optimization_results[query_name] = {
                'baseline_time': baseline_time,
                'optimized_time': optimized_time,
                'comparison': asdict(comparison),
                'optimizations_applied': [opt.value for opt in optimization_result.optimizations_applied],
                'explanations': optimization_result.explanation
            }
        
        return optimization_results
    
    def _benchmark_query_caching(self) -> Dict[str, Any]:
        """Benchmark query result caching effectiveness."""
        caching_results = {}
        
        # Clear cache first
        self.query_cache.clear()
        
        # Test each query with cache miss and hit scenarios
        for query_name, query in self.test_queries.items():
            print(f"\nðŸ§ª Testing caching for {query_name}")
            
            # First execution (cache miss)
            start_time = time.time()
            cached_result = self.query_cache.get(query)
            if cached_result is None:
                # Simulate query execution and cache result
                mock_result = self._execute_mock_query(query)
                self.query_cache.put(query, mock_result)
                cache_miss_time = time.time() - start_time
                print(f"   Cache miss: {cache_miss_time:.3f}s")
            else:
                cache_miss_time = 0.0
            
            # Second execution (cache hit)
            start_time = time.time()
            cached_result = self.query_cache.get(query)
            cache_hit_time = time.time() - start_time
            print(f"   Cache hit: {cache_hit_time:.3f}s")
            
            # Calculate improvement
            if cache_miss_time > 0:
                cache_speedup = cache_miss_time / cache_hit_time if cache_hit_time > 0 else float('inf')
                print(f"   Cache speedup: {cache_speedup:.1f}x")
            else:
                cache_speedup = 1.0
            
            caching_results[query_name] = {
                'cache_miss_time': cache_miss_time,
                'cache_hit_time': cache_hit_time,
                'cache_speedup': cache_speedup
            }
        
        # Get cache statistics
        cache_stats = self.query_cache.get_stats()
        caching_results['cache_stats'] = cache_stats
        
        return caching_results
    
    def _benchmark_join_performance(self) -> Dict[str, Any]:
        """Benchmark JOIN operation optimizations."""
        join_results = {}
        
        join_queries = {
            'simple_join': self.test_queries['simple_join'],
            'complex_join': self.test_queries['complex_join']
        }
        
        for query_name, query in join_queries.items():
            print(f"\nðŸ§ª Testing JOIN optimization for {query_name}")
            
            # Analyze query complexity
            complexity = self.sql_optimizer.analyze_query_complexity(query)
            print(f"   Complexity score: {complexity['complexity_score']}")
            print(f"   JOIN count: {complexity['join_count']}")
            
            # Test original vs optimized
            baseline_time = self._time_mock_query_execution(query)
            
            optimization_result = self.sql_optimizer.optimize_query(query)
            optimized_time = self._time_mock_query_execution(optimization_result.optimized_query)
            
            comparison = PerformanceComparison(baseline_time, optimized_time)
            
            print(f"   Original: {baseline_time:.3f}s")
            print(f"   Optimized: {optimized_time:.3f}s")
            print(f"   Improvement: {comparison.get_improvement_level()}")
            
            join_results[query_name] = {
                'complexity': complexity,
                'baseline_time': baseline_time,
                'optimized_time': optimized_time,
                'comparison': asdict(comparison)
            }
        
        return join_results
    
    def _benchmark_large_results(self) -> Dict[str, Any]:
        """Benchmark handling of large result sets."""
        large_result_tests = {}
        
        # Test query that would return many rows
        large_query = self.test_queries['large_result_set']
        
        print(f"\nðŸ§ª Testing large result set handling")
        
        # Test without FIRST clause
        baseline_time = self._time_mock_query_execution(large_query)
        print(f"   Without FIRST: {baseline_time:.3f}s")
        
        # Test with FIRST clause optimization
        optimization_result = self.sql_optimizer.optimize_query(large_query)
        optimized_time = self._time_mock_query_execution(optimization_result.optimized_query)
        print(f"   With FIRST 1000: {optimized_time:.3f}s")
        
        comparison = PerformanceComparison(baseline_time, optimized_time)
        print(f"   Improvement: {comparison.get_improvement_level()}")
        
        large_result_tests['large_result_set'] = {
            'baseline_time': baseline_time,
            'optimized_time': optimized_time,
            'comparison': asdict(comparison),
            'optimization_applied': 'FIRST clause limiting'
        }
        
        return large_result_tests
    
    def _time_mock_query_execution(self, query: str) -> float:
        """
        Mock query execution timing based on query complexity.
        
        This simulates database execution time based on query characteristics
        since we can't run actual queries without database permissions.
        """
        # Simulate execution time based on query complexity
        base_time = 0.001  # 1ms base time
        
        query_upper = query.upper()
        
        # Add time based on query characteristics
        if 'JOIN' in query_upper:
            join_count = query_upper.count('JOIN')
            base_time += join_count * 0.01  # 10ms per JOIN
        
        if 'WHERE' not in query_upper:
            base_time += 0.05  # 50ms penalty for full table scan
        
        if 'LIKE' in query_upper:
            base_time += 0.02  # 20ms for string pattern matching
        
        if 'COUNT(*)' in query_upper:
            base_time += 0.005  # 5ms for aggregation
        
        if 'ORDER BY' in query_upper:
            base_time += 0.01  # 10ms for sorting
        
        # Add some randomness to simulate real-world variation
        import random
        variation = random.uniform(0.8, 1.2)
        
        # Simulate the actual execution
        time.sleep(base_time * variation)
        
        return base_time * variation
    
    def _execute_mock_query(self, query: str) -> List[Tuple]:
        """Execute mock query and return simulated results."""
        # Return mock results based on query type
        query_upper = query.upper()
        
        if 'COUNT(*)' in query_upper:
            return [("count", 1234)]
        elif 'BNAME' in query_upper:
            return [("BNAME", "MÃ¼ller"), ("BNAME", "Schmidt"), ("BNAME", "Weber")]
        else:
            return [("result", "mock_data")]
    
    def _generate_summary(self, test_categories: Dict[str, Any]) -> Dict[str, Any]:
        """Generate benchmark summary."""
        summary = {
            'total_tests': sum(len(category) for category in test_categories.values()),
            'optimization_effectiveness': {},
            'overall_performance_gain': 0.0
        }
        
        # Calculate average improvement across all optimizations
        all_improvements = []
        
        for category_name, category_results in test_categories.items():
            if category_name == 'query_caching':
                continue  # Handle caching separately
            
            category_improvements = []
            for test_name, test_result in category_results.items():
                if 'comparison' in test_result:
                    improvement = test_result['comparison']['improvement_factor']
                    category_improvements.append(improvement)
                    all_improvements.append(improvement)
            
            if category_improvements:
                avg_improvement = statistics.mean(category_improvements)
                summary['optimization_effectiveness'][category_name] = {
                    'average_improvement': avg_improvement,
                    'best_improvement': max(category_improvements),
                    'test_count': len(category_improvements)
                }
        
        if all_improvements:
            summary['overall_performance_gain'] = statistics.mean(all_improvements)
        
        return summary
    
    def _generate_recommendations(self, test_categories: Dict[str, Any]) -> List[str]:
        """Generate optimization recommendations based on benchmark results."""
        recommendations = []
        
        # Analyze SQL optimization results
        sql_results = test_categories.get('sql_optimization', {})
        high_impact_optimizations = [
            test for test, result in sql_results.items()
            if result.get('comparison', {}).get('improvement_factor', 1.0) > 2.0
        ]
        
        if high_impact_optimizations:
            recommendations.append(
                f"ðŸš€ HIGH IMPACT: SQL optimizations showed >2x improvement for {len(high_impact_optimizations)} queries. "
                f"Focus on: {', '.join(high_impact_optimizations)}"
            )
        
        # Analyze caching effectiveness
        cache_results = test_categories.get('query_caching', {})
        cache_stats = cache_results.get('cache_stats', {})
        hit_rate = cache_stats.get('hit_rate_percent', 0)
        
        if hit_rate > 50:
            recommendations.append(
                f"ðŸ’¾ EXCELLENT: Query caching shows {hit_rate:.1f}% hit rate. "
                f"Consider increasing cache size for even better performance."
            )
        elif hit_rate > 25:
            recommendations.append(
                f"ðŸ’¾ GOOD: Query caching shows {hit_rate:.1f}% hit rate. "
                f"Consider optimizing cache TTL settings."
            )
        else:
            recommendations.append(
                f"ðŸ’¾ IMPROVE: Query caching shows only {hit_rate:.1f}% hit rate. "
                f"Review query patterns and cache configuration."
            )
        
        # Analyze JOIN performance
        join_results = test_categories.get('join_performance', {})
        complex_joins = [
            test for test, result in join_results.items()
            if result.get('complexity', {}).get('join_count', 0) > 2
        ]
        
        if complex_joins:
            recommendations.append(
                f"ðŸ”— JOIN OPTIMIZATION: {len(complex_joins)} queries have >2 JOINs. "
                f"Consider denormalization or indexed views for better performance."
            )
        
        # General recommendations
        recommendations.extend([
            "ðŸ“Š Monitor query performance regularly using the benchmarking suite",
            "ðŸ”§ Apply SQL optimizations incrementally and measure impact",
            "ðŸ’¾ Implement query result caching for frequently accessed data",
            "ðŸŽ¯ Focus optimization efforts on queries with highest execution frequency"
        ])
        
        return recommendations
    
    def print_detailed_report(self, results: Dict[str, Any]):
        """Print detailed benchmark report."""
        print("\n" + "=" * 80)
        print("ðŸ“‹ DETAILED PERFORMANCE BENCHMARK REPORT")
        print("=" * 80)
        
        # Summary
        summary = results['summary']
        print(f"\nðŸ“Š SUMMARY:")
        print(f"   Total tests: {summary['total_tests']}")
        print(f"   Overall performance gain: {summary['overall_performance_gain']:.1f}x")
        
        # Category effectiveness
        for category, effectiveness in summary['optimization_effectiveness'].items():
            print(f"   {category.replace('_', ' ').title()}: {effectiveness['average_improvement']:.1f}x average improvement")
        
        # Recommendations
        print(f"\nðŸŽ¯ RECOMMENDATIONS:")
        for i, recommendation in enumerate(results['recommendations'], 1):
            print(f"   {i}. {recommendation}")
        
        print(f"\nâœ… Benchmark completed at {results['timestamp']}")


def run_comprehensive_database_benchmark():
    """Run comprehensive database performance benchmark."""
    benchmark = DatabasePerformanceBenchmark()
    results = benchmark.run_comprehensive_benchmark()
    benchmark.print_detailed_report(results)
    return results


if __name__ == "__main__":
    results = run_comprehensive_database_benchmark()